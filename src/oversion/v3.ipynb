{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">\n",
       "ioDownload = (fName, txt) => {\n",
       "    var element = document.createElement('a');\n",
       "    element.setAttribute('href', 'data:text/plain;charset=utf-8,' + encodeURIComponent(txt));\n",
       "    element.setAttribute('download', fName);\n",
       "\n",
       "    element.style.display = 'none';\n",
       "    element.click();\n",
       "}\n",
       "\n",
       "ioJupOutput = (outType, txtType, txt) => {\n",
       "  let output = {\n",
       "    'output_type': outType\n",
       "  }\n",
       "  if (outType == 'stream') {\n",
       "    output['name'] = 'stdout'\n",
       "    output['text'] = [txt]\n",
       "  }\n",
       "  else {\n",
       "    output['data'] = {}\n",
       "    output['data'][txtType] = [txt]\n",
       "  }\n",
       "  return output\n",
       "}\n",
       "\n",
       "ioJoinStr = (txtArr, sep) => {\n",
       "  let result = ''\n",
       "  let isSkip = true\n",
       "  txtArr.forEach((txt, i) => {\n",
       "    if (txt.trim().length == 0 && isSkip == true)\n",
       "      return\n",
       "    isSkip = false\n",
       "    result += txt\n",
       "    if (i < txtArr.length - 1)\n",
       "      result += sep\n",
       "  })\n",
       "  return result\n",
       "}\n",
       "\n",
       "ioGetOutput = (codeNode) => {\n",
       "  let n = codeNode\n",
       "  while (n.classList.length != 0)\n",
       "    n = n.parentNode\n",
       "  outputNodes = n.querySelectorAll('.tableDisplay')\n",
       "  let outputs = []\n",
       "  outputNodes.forEach(n => {\n",
       "    let txtNode = n.querySelector('.plainTextContent')\n",
       "    if (txtNode != null) {\n",
       "      outputs.push({\n",
       "        'outType': 'stream',\n",
       "        'dType': null,\n",
       "        'txt': txtNode.innerText\n",
       "      })\n",
       "      return\n",
       "    }\n",
       "    let htmlNode = n.querySelector('.resultContained')\n",
       "    if (htmlNode != null)\n",
       "      outputs.push({\n",
       "        'outType': 'execute_result',\n",
       "        'dType': 'text/html',\n",
       "        'txt': htmlNode.innerHTML\n",
       "      })\n",
       "  })\n",
       "  return outputs\n",
       "}\n",
       "\n",
       "ioJupDownload = () => {\n",
       "    codeContainers = document.querySelectorAll('.ace_layer.ace_text-layer')\n",
       "\n",
       "    let txtCellTempl = {\n",
       "        \"cell_type\": null,\n",
       "        \"execution_count\": 0,\n",
       "        \"metadata\": {},\n",
       "        \"outputs\": [\n",
       "        ],\n",
       "        \"source\": [\n",
       "        ]\n",
       "    }\n",
       "\n",
       "    let jup = {\n",
       "        \"cells\": [],\n",
       "        \"metadata\": {\n",
       "        },\n",
       "        \"nbformat\": 4,\n",
       "        \"nbformat_minor\": 2\n",
       "    }\n",
       "\n",
       "    codeContainers.forEach(container => {\n",
       "        let codes = []\n",
       "        container.childNodes.forEach(lineGroup => {\n",
       "            let line = []\n",
       "            lineGroup.childNodes.forEach((o, i) => {\n",
       "              if (i > 0)\n",
       "                line.push(o.innerText.trim())\n",
       "              else\n",
       "                line.push(o.innerText)\n",
       "            })\n",
       "            codes.push(line.join(''))\n",
       "        })\n",
       "        if (codes.length == 0)\n",
       "          return\n",
       "        let txtCell = structuredClone(txtCellTempl)\n",
       "        if (codes[0].length > 0 && codes[0][0] != '%') {\n",
       "            codes = ioJoinStr(codes, '\\n')\n",
       "            txtCell['cell_type'] = 'code'\n",
       "        }\n",
       "        else if (codes[0].length >= 8 && codes[0].substring(0, 8) == '%pyspark') {\n",
       "            codes = ioJoinStr(codes.slice(1), '\\n')\n",
       "            txtCell['cell_type'] = 'code'\n",
       "        }\n",
       "        else if (codes[0].length == 3 && codes[0].substring(0, 3) == '%md') {\n",
       "            codes = ioJoinStr(codes.slice(1), '\\n')\n",
       "            txtCell['cell_type'] = 'markdown'\n",
       "        }\n",
       "        else return\n",
       "\n",
       "        if (txtCell['cell_type'] == 'code') {\n",
       "          ioGetOutput(container).forEach(o => {\n",
       "            txtCell['outputs'].push(ioJupOutput(o['outType'], o['dType'], o['txt']))\n",
       "          })\n",
       "        }\n",
       "        txtCell['source'].push(codes)\n",
       "        jup['cells'].push(txtCell)\n",
       "    })\n",
       "\n",
       "    let fName = document.querySelector('.notebook-actionBar-title').innerText.replace('/', '_') + '.ipynb'\n",
       "    ioDownload(fName, JSON.stringify(jup))\n",
       "}\n",
       "  </script><button class=\"btn btn-primary\" onclick=\"ioJupDownload()\">Download ipynb</button>\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.jup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CPU---\n",
      "#0: 3.2%\t#1: 0.0%\t#2: 3.1%\n",
      "#3: 0.0%\t#4: 1.6%\t#5: 0.0%\n",
      "#6: 3.1%\t#7: 0.0%\t#8: 0.0%\n",
      "#9: 0.0%\t#10: 1.6%\t#11: 0.0%\n",
      "#12: 1.6%\t#13: 0.0%\t#14: 0.0%\n",
      "#15: 0.0%\t#16: 0.0%\t#17: 0.0%\n",
      "#18: 0.0%\t#19: 0.0%\n",
      "\n",
      "---Memory---\n",
      "29252MiB (44%) free, 65237MiB total\n",
      "\n",
      "---GPU---\n",
      "#0. Tesla P100-PCIE-16GB\n",
      "memory: 5323MiB (32%) free, 16287MiB total\n",
      "Free VRAM: 5323MB\n",
      "Optimize n_layer: 22\n"
     ]
    }
   ],
   "source": [
    "# Lựa chọn n_layer tối ưu. Dựa trên dung lượng VRAM đang trống. \n",
    "# Với Gemma 2 9b, Mỗi layer chiếm khoảng 240MB \n",
    "\n",
    "# Xóa llm \n",
    "if 'llm' in locals(): del llm\n",
    "if 'gemma_2_model' in locals(): del gemma_2_model\n",
    "\n",
    "# Lấy VRAM còn trống \n",
    "_, free_GPUs = z.showUsage()\n",
    "print(f'Free VRAM: {free_GPUs[0]}MB')\n",
    "\n",
    "n_layer = int(free_GPUs[0] / 240)\n",
    "\n",
    "if n_layer == 0:\n",
    "    print(f'Sufficient VRAM (n_layer=={n_layer}), use CPU')\n",
    "    n_layer = None\n",
    "else:\n",
    "    if n_layer > 43:\n",
    "        n_layer = 43\n",
    "    print(f'Optimize n_layer: {n_layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zeppelin\\bin\\python\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:161: UserWarning: Field \"model_path\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zeppelin\\bin\\python\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zeppelin\\bin\\python\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uvicorn\n",
    "import asyncio\n",
    "import multiprocessing\n",
    "\n",
    "from uvicorn import Config, Server\n",
    "from fastapi import FastAPI, File, UploadFile, Form\n",
    "from fastapi.responses import JSONResponse, StreamingResponse, FileResponse\n",
    "\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_qdrant import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from tempfile import NamedTemporaryFile\n",
    "from httpx import TimeoutException\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from werkzeug.datastructures import FileStorage\n",
    "from fpdf import FPDF\n",
    "\n",
    "GEMMA2_MODEL = \"soc-models/gemma-2-9b-it-Q6_K_L.gguf\"\n",
    "EMBEDDINGS_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# EMBEDDINGS_MODEL = \"dunzhang/stella_en_1.5B_v5\"\n",
    "\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=EMBEDDINGS_MODEL,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "gemma_2_model = ChatLlamaCpp(\n",
    "    model_path=GEMMA2_MODEL,\n",
    "    verbose=False, \n",
    "    temperature=0,\n",
    "    n_ctx=8192,  \n",
    "    max_tokens=1024,  \n",
    "    f16_kv=False,  \n",
    "    n_gpu_layers=n_layer,  \n",
    "    n_threads=multiprocessing.cpu_count()-1,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "app = FastAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Start---llms response----\n",
    "\n",
    "def process_template(template: str) -> str:\n",
    "    template = template.replace(\"{\", \"{{\")\n",
    "    template = template.replace(\"}\", \"}}\")\n",
    "    return template\n",
    "\n",
    "def create_prompt(template: str) -> PromptTemplate:\n",
    "    processed_template = process_template(template)\n",
    "    \n",
    "    return PromptTemplate(\n",
    "        template=processed_template,\n",
    "        input_variables=[]\n",
    "    )\n",
    "\n",
    "def get_chain(template: str):\n",
    "    prompt = create_prompt(template)\n",
    "    \n",
    "    chain = (\n",
    "        prompt \n",
    "        | gemma_2_model \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return chain\n",
    "    \n",
    "async def generate_result(template: str) -> str:\n",
    "    chain = get_chain(template)\n",
    "    response = await chain.ainvoke({})\n",
    "    return response\n",
    "\n",
    "async def generate_stream_json(template: str) -> str:\n",
    "    chain = get_chain(template)\n",
    "    \n",
    "    async for chunk in chain.astream({}):\n",
    "        yield json.dumps({\n",
    "            \"response\": chunk,\n",
    "            \"success\": True\n",
    "        }) + \"\\n\"\n",
    "\n",
    "async def generate_stream_str(template: str) -> str:\n",
    "    chain = get_chain(template)\n",
    "    \n",
    "    async for chunk in chain.astream({}):\n",
    "        yield chunk\n",
    "\n",
    "#---End---llms response----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Start---query----\n",
    "\n",
    "DASHBOARD = \n",
    "QDRANT_URL = \n",
    "QDRANT_API_KEY = \n",
    "\n",
    "# COLLECTION_NAME = \"DATA-SOC\"\n",
    "COLLECTION_NAME = \"DATA-SOC-SEMANTIC\"\n",
    "\n",
    "CONTENT_PAYLOAD_KEY = \"content\"\n",
    "METADATA_PAYLOAD_KEY = \"metadata\"\n",
    "\n",
    "SAVE_PATH = './soc-data-store'\n",
    "BATCH_SIZE_UPLOAD = 10\n",
    "\n",
    "TOP_K = 3\n",
    "MAX_SAME_QUERY = 1\n",
    "MAX_DOCS_FOR_CONTEXT = 5\n",
    "\n",
    "def collection_exists(client: QdrantClient, collection_name: str) -> bool:\n",
    "    \"\"\"Check if a Qdrant collection exists\"\"\"\n",
    "    collections = client.get_collections().collections\n",
    "    return any(col.name == collection_name for col in collections)\n",
    "\n",
    "def existing_collection(collection_name: str) -> Qdrant:\n",
    "    \"\"\"Create vector retriever\"\"\"\n",
    "    \n",
    "    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "    if not collection_exists(client, collection_name):\n",
    "        return None\n",
    "\n",
    "    doc_store = Qdrant.from_existing_collection(\n",
    "        url=QDRANT_URL,\n",
    "        api_key=QDRANT_API_KEY,\n",
    "        embedding=embeddings,\n",
    "        collection_name=collection_name,    \n",
    "        content_payload_key=CONTENT_PAYLOAD_KEY,\n",
    "        metadata_payload_key=METADATA_PAYLOAD_KEY\n",
    "    )\n",
    "    return doc_store\n",
    "\n",
    "def similarity_search(para: dict) -> list[Document]:\n",
    "    \"\"\"RRF retriever\"\"\"\n",
    "    common_doc_store = existing_collection(COLLECTION_NAME)\n",
    "    user_doc_store = existing_collection(para[\"user_id\"])\n",
    "    \n",
    "    all_results = []\n",
    "    if common_doc_store:\n",
    "        common_results = common_doc_store.similarity_search_with_score(para[\"user_query\"], k=MAX_DOCS_FOR_CONTEXT)\n",
    "        all_results.extend(common_results)\n",
    "        \n",
    "    if user_doc_store:\n",
    "        user_results = user_doc_store.similarity_search_with_score(para[\"user_query\"], k=MAX_DOCS_FOR_CONTEXT)\n",
    "        all_results.extend(user_results)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "async def get_ask_template(user_id: str, user_query: str) -> str:\n",
    "    ssearch = RunnableLambda(similarity_search)\n",
    "    context = await ssearch.ainvoke({\"user_id\": user_id, \"user_query\": user_query})\n",
    "    context = [c[0].page_content for c in context]\n",
    "    question = user_query\n",
    "    \n",
    "    template = f\"\"\"\n",
    "        You are a support chatbot providing information about information security and network security.\n",
    "        Please answer the following question based on the information provided and retrieve the relevant links, names of images, tables, pages, sources of the content contained in the information:\n",
    "\n",
    "        Information: {context}\n",
    "        Question: {question}\n",
    "        The final answer must be translated into Vietnamese with structure:\n",
    "            <no-content>\n",
    "            #heading1: <content>\n",
    "                <no-content>\n",
    "                ##heading1.1: <content>\n",
    "                    <no-content>\n",
    "                    ###heading1.1.1: <content>\n",
    "                        <no-content>\n",
    "                        ####heading1.1.1.1: <content>\n",
    "                            <no-content>\n",
    "                        <no-content>\n",
    "                    <no-content>\n",
    "                    ###heading1.1.2: <content>\n",
    "                        <no-content>\n",
    "                    <no-content>\n",
    "                <no-content>\n",
    "            <no-content>\n",
    "            #heading2: <content>\n",
    "                <no-content>\n",
    "                #heading2.1: <content>\n",
    "                    <no-content>\n",
    "                <no-content>\n",
    "            <no-content>\n",
    "            #heading3: <content>\n",
    "                <no-content>\n",
    "                ##heading3.1: <content>\n",
    "                    <no-content>\n",
    "                <no-content>\n",
    "                ##heading3.2: <content>\n",
    "                    <no-content>\n",
    "                <no-content>\n",
    "            <no-content>\n",
    "            **images**: https://example.com/images1, https://example.com/images2, ...\n",
    "            **tables**: https://example.com/tables1, https://example.com/tables2, ...\n",
    "            **pages**: https://example.com/pages1, https://example.com/pages2, ...\n",
    "            **sources**: https://example.com/sources1, https://example.com/sources2, ...\n",
    "        \n",
    "        Question example: Xác thực 2 yếu tố là gì?\n",
    "        Output example: \n",
    "            #heading1: Xác thực hai yếu tố (2FA) là một biện pháp bảo mật bổ sung yêu cầu người dùng cung cấp hai hình thức xác minh khác nhau để đăng nhập vào tài khoản của họ.\n",
    "            #heading2: Các loại xác thực 2FA\n",
    "                ##heading2.1: Yếu tố đầu tiên: Thông thường là tên người dùng và mật khẩu.\n",
    "                ##heading2.2: Yếu tố thứ hai: Có thể bao gồm:\n",
    "                    ###heading2.2.1: Mã OTP (One-Time Password) được gửi qua SMS hoặc email.\n",
    "                    ###heading2.2.2: Ứng dụng xác thực trên điện thoại di động (ví dụ: Google Authenticator, Authy).\n",
    "                    ###heading2.2.3: Dòng chữ ký vật lý (FIDO2).\n",
    "                    ###heading2.2.4: Chuyển đổi sinh trắc học (nhận dạng khuôn mặt, vân tay).\n",
    "            #heading3: Lợi ích của Xác thực 2FA\n",
    "                ##heading3.1: Xác thực 2FA giúp tăng cường bảo mật tài khoản bằng cách làm cho việc truy cập trái phép trở nên khó khăn hơn đáng kể. Bằng cách yêu cầu hai yếu tố xác minh, 2FA ngăn chặn kẻ tấn công có được quyền truy cập vào tài khoản của bạn ngay cả khi họ biết tên người dùng vàmật khẩu của bạn.\n",
    "            **sources**: https://www.idrbt.ac.in/wp-content/uploads/2022/07/ISOC.pdf\n",
    "    \"\"\"\n",
    "    return template\n",
    "\n",
    "#---End---Query----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Start---evaluate----\n",
    "\n",
    "n_TODO = 31\n",
    "TODO_LIST = [\n",
    "    \"\"\"**Review and update information security policies**\"\"\",\n",
    "    \"\"\"**Clearly define the policy change approval process**\"\"\",\n",
    "    \"\"\"**Assess risks from third-party service providers**\"\"\",\n",
    "    \"\"\"**Establish agreements with third-party service providers**\"\"\",\n",
    "    \"\"\"**Set up a formal risk management program**\"\"\",\n",
    "    \"\"\"**Develop an ethics violation handling process**\"\"\",\n",
    "    \"\"\"**Review and update access management procedures**\"\"\",\n",
    "    \"\"\"**Provide details on logical access controls**\"\"\",\n",
    "    \"\"\"**Enhance physical access controls**\"\"\",\n",
    "    \"\"\"**Upgrade power backup systems**\"\"\",\n",
    "    \"\"\"**Conduct regular penetration testing**\"\"\",\n",
    "    \"\"\"**Provide details on incident management, incident response, and change management procedures**\"\"\",\n",
    "    \"\"\"**Configure disaster recovery for production servers**\"\"\",\n",
    "    \"\"\"**Improve patch management procedures**\"\"\",\n",
    "    \"\"\"**Enhance security monitoring**\"\"\",\n",
    "    \"\"\"**Improve performance monitoring**\"\"\",\n",
    "    \"\"\"**Enhance the ticketing system**\"\"\",\n",
    "    \"\"\"**Perform more frequent vulnerability scanning**\"\"\",\n",
    "    \"\"\"**Improve intrusion detection and prevention**\"\"\",\n",
    "    \"\"\"**Configure and manage firewalls**\"\"\",\n",
    "    \"\"\"**Provide details on backup and data recovery procedures**\"\"\",\n",
    "    \"\"\"**Enhance recovery testing**\"\"\",\n",
    "    \"\"\"**Establish an IT asset management process**\"\"\",\n",
    "    \"\"\"**Improve vendor management processes**\"\"\",\n",
    "    \"\"\"**Enhance employee security awareness training**\"\"\",\n",
    "    \"\"\"**Provide security training for customers**\"\"\",\n",
    "    \"\"\"**Improve recruitment, training, and termination processes**\"\"\",\n",
    "    \"\"\"**Perform operational effectiveness testing**\"\"\",\n",
    "    \"\"\"**Consider SOC 2 Type 2 audit**\"\"\",\n",
    "    \"\"\"**Ensure compliance with security regulations and standards**\"\"\",\n",
    "    \"\"\"**Conduct regular security assessments**)\"\"\"\n",
    "]\n",
    "\n",
    "TODO_LIST_STR = \"\\n\".join([f\"{i + 1}. {item}\" for i, item in enumerate(TODO_LIST)])\n",
    "\n",
    "def process_result_of_evaluate(result: str) -> list[int]:\n",
    "    newresult = ''.join((ch if ch in '0123456789.-e' else ' ') for ch in result)\n",
    "    listOfNumbers = [int(i) for i in newresult.split() if i.isdigit() and int(i) > 0 and int(i) <= n_TODO]\n",
    "    return listOfNumbers\n",
    "\n",
    "def get_evaluate_template(todo_list_str: str, content: str) -> dict:\n",
    "    \"\"\"Query with vector db\"\"\"\n",
    "    \n",
    "    template = f\"\"\"\n",
    "        Please evaluate the security-related content based on the \"content\" provided, which definition and level of requirements are satisfied in the \"To-do List\":\n",
    "\n",
    "        To-do List: {todo_list_str}\n",
    "        Content: {content}\n",
    "        The final answer is the positions in the to-do list, only the positions are given, if there is no result or cannot be evaluated or the result is ambiguous and unclear during the evaluation, the answer must be: -1\n",
    "        Output example 1: 1, 2, 3\n",
    "        Output example 2: 3, 7, 12, 23\n",
    "        Output example 3: -1 \n",
    "    \"\"\"\n",
    "\n",
    "    return template\n",
    "\n",
    "#---End---evaluate----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Start---save file----\n",
    "\n",
    "def check_file_type(file_path: str):\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    if file_extension.lower() == \".pdf\":\n",
    "        return \"PDF\"\n",
    "    elif file_extension.lower() == \".docx\":\n",
    "        return \"DOCX\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def get_file_name(file_path: str):\n",
    "    file_name = file_path.split('\\\\')[-1]\n",
    "    folder_path = file_path.split('\\\\')[:-1]\n",
    "    return file_name\n",
    "\n",
    "def get_folder_abs_path(user_id: str) -> str:\n",
    "    if not os.path.exists(SAVE_PATH):\n",
    "        os.makedirs(SAVE_PATH)\n",
    "\n",
    "    folder = SAVE_PATH + \"/\" + user_id\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    folder_abs_path = os.path.abspath(folder)\n",
    "    return folder_abs_path\n",
    "\n",
    "async def save_pdf(file: str, user_id: str) -> str:\n",
    "    folder_abs_path = get_folder_abs_path(user_id)\n",
    "    \n",
    "    pdf_content = await file.read()\n",
    "    file_name = file.filename\n",
    "    file_abs_path = os.path.join(folder_abs_path, file_name)\n",
    "\n",
    "    with open(file_abs_path, \"wb\") as output_file:\n",
    "        output_file.write(pdf_content)\n",
    "    \n",
    "    return file_abs_path\n",
    "    \n",
    "def get_check_is_socreport_template(sample_content: str):\n",
    "    template = f\"\"\"\n",
    "        Are you a chatbot that helps classify whether the provided \"content sample\" is part of the \"SOC report\" content?\n",
    "        \n",
    "        Content sample: {sample_content}\n",
    "        \n",
    "        **No additional content is generated.**\n",
    "        **The final answer is True or False.**\n",
    "        **If there is no result or cannot be evaluated or the result is unclear and ambiguous during the evaluation, the answer must be: False**\n",
    "        \n",
    "        Example output 1: True\n",
    "        Example output 2: False\n",
    "        Example output 3: True\n",
    "    \"\"\"\n",
    "    return template\n",
    "    \n",
    "async def check_is_socreport_file(sample_content: str):\n",
    "    template = get_check_is_socreport_template(sample_content)\n",
    "    result = await generate_result(template)\n",
    "    result = result.lower().strip()\n",
    "    print(result)\n",
    "    \n",
    "    if result.lower() == \"true\": return True\n",
    "    return False\n",
    "    \n",
    "    \n",
    "#---End---save file----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Start---create file PDF----\n",
    "\n",
    "def initialize_pdf():\n",
    "    pdf = FPDF('P', 'mm', 'A4')\n",
    "    pdf.add_page()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_font(\"DejaVu\", \"\", \"soc-fonts/DejaVuSans.ttf\", uni=True)\n",
    "    pdf.add_font(\"DejaVu-Bold\", \"\", \"soc-fonts/DejaVuSans-Bold.ttf\", uni=True)\n",
    "    pdf.add_font(\"DejaVu-Italic\", \"\", \"soc-fonts/DejaVuSerif-Italic.ttf\", uni=True)\n",
    "    return pdf\n",
    "\n",
    "def add_header(pdf, title:str):\n",
    "    pdf.set_font(\"Arial\", 'B', 16)\n",
    "    pdf.cell(0, 10, title, 0, 1, 'C')\n",
    "\n",
    "def add_section_title(pdf, title:str):\n",
    "    pdf.set_font(\"Arial\", 'B', 14)\n",
    "    pdf.cell(0, 10, title, 0, 1)\n",
    "\n",
    "def add_task_list(pdf, todo_list:list[str], indices:dict, title:str):\n",
    "    add_section_title(pdf, title)\n",
    "    pdf.set_font(\"DejaVu\", '', 13)\n",
    "    line_height = 10\n",
    "    \n",
    "    for i, (key, value) in enumerate(indices.items()):\n",
    "        if key < 0 or key >= len(todo_list): continue\n",
    "    \n",
    "        checkbox = \"\\u2611\" if value else \"\\u2610\"\n",
    "        title_number = \"1\" if value else \"2\"\n",
    "        parts = todo_list[key].split(\"**\")\n",
    "        \n",
    "        pdf.set_x(pdf.get_x() + 10)\n",
    "        pdf.set_font(\"DejaVu-Bold\", '', 13)\n",
    "        pdf.multi_cell(0, line_height, f\"{title_number}.{i+1}. {checkbox} {parts[1].strip()}\" , 0)\n",
    "\n",
    "        # pdf.set_x(pdf.get_x() + 10)\n",
    "        # pdf.set_font(\"DejaVu\", '', 13)\n",
    "        # pdf.multi_cell(0, line_height, f\"\\u279B{parts[2].strip()}\", 0)\n",
    "        \n",
    "        # if value:\n",
    "        #     pdf.set_x(pdf.get_x() + 10)\n",
    "        #     pdf.set_font(\"DejaVu-Italic\", '', 13)\n",
    "        #     pdf.multi_cell(0, line_height, f\"\\u201C\\u2026{value.page_content.strip()}\\u2026\\u201D\", 0)\n",
    "            \n",
    "        pdf.ln(5)\n",
    "\n",
    "def create_information_security_evaluation(todo_list:list[str], completed_indices:dict, user_id:str, file_evaluation_name:str=\"Information_Security_Evaluation.pdf\") -> str:\n",
    "    folder_abs_path = get_folder_abs_path(user_id)\n",
    "    output_path = os.path.join(folder_abs_path, file_evaluation_name)\n",
    "    \n",
    "    pdf = initialize_pdf()\n",
    "    add_header(pdf, \"Information Security Evaluation\")\n",
    "    \n",
    "    add_task_list(pdf, todo_list, completed_indices, \"1. Completed Tasks\",)\n",
    "    pdf.add_page()\n",
    "    \n",
    "    uncompleted_positions = set(range(len(todo_list))) - set(completed_indices.keys())\n",
    "    uncompleted_positions = {pos: None for pos in uncompleted_positions}\n",
    "    add_task_list(pdf , todo_list, uncompleted_positions, \"2. Pending Tasks\")\n",
    "    \n",
    "    pdf.output(output_path)\n",
    "    return file_evaluation_name, output_path\n",
    "\n",
    "#---End---create file PDF----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Start---document----\n",
    "\n",
    "def get_raw_documents(file_path: str) -> list[Document]:\n",
    "    file_type = check_file_type(file_path)\n",
    "    if file_type == \"PDF\":\n",
    "        raw_documents = PyPDFLoader(file_path).load()\n",
    "    elif file_type == \"DOCX\":\n",
    "        raw_documents = Docx2txtLoader(file_path).load()\n",
    "    else:\n",
    "        raw_documents = []\n",
    "    return raw_documents\n",
    "    \n",
    "def process_raw_documents(raw_documents: list[Document]) -> list[Document]: \n",
    "    source = get_file_name(raw_documents[0].metadata.get('source'))\n",
    "    for idx, raw_document in enumerate(raw_documents):\n",
    "        page = raw_document.metadata.get('page')\n",
    "        metadata = {'source': source, 'page': page}\n",
    "        setattr(raw_document, 'metadata', metadata)\n",
    "    return raw_documents\n",
    "    \n",
    "def get_documents(file_path: str) -> list[Document]:\n",
    "    raw_documents = get_raw_documents(file_path)\n",
    "    documents = process_raw_documents(raw_documents)\n",
    "    return documents\n",
    "    \n",
    "def get_texts(file_path: str) -> (str, str):\n",
    "    raw_documents = get_raw_documents(file_path)\n",
    "    source = raw_documents[0].metadata.get('source').split('\\\\')[-1]\n",
    "    page_content = \"\"\n",
    "    for raw_document in raw_documents:\n",
    "        page_content += raw_document.page_content\n",
    "    return source, page_content\n",
    "\n",
    "def process_documents_before_store(documents: list[Document]) -> list[Document]:\n",
    "    pre_page = 0\n",
    "    for index, document in enumerate(documents):\n",
    "        size = len(document.page_content)\n",
    "        metadata = document.metadata\n",
    "        metadata['size'] = size\n",
    "        metadata['index'] = index\n",
    "        metadata['pre_page'] = pre_page\n",
    "        pre_page = metadata.get('page')\n",
    "        setattr(document, 'metadata', metadata)\n",
    "        \n",
    "    return documents\n",
    "\n",
    "def split_documents_file_common(file_path: str) -> list[Document]:\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings=embeddings, \n",
    "        breakpoint_threshold_type=\"percentile\", \n",
    "        number_of_chunks=1024\n",
    "    )\n",
    "    \n",
    "    documents = get_documents(file_path)\n",
    "    documents = text_splitter.split_documents(documents)\n",
    "    documents = process_documents_before_store(documents)\n",
    "    return documents\n",
    "    \n",
    "def split_documents_file_soc(file_path: str) -> list[Document]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 6144, \n",
    "        chunk_overlap = 124,\n",
    "        separators = [\n",
    "            \"\\n\\n\",\n",
    "            \"\\n\",\n",
    "            \" \",\n",
    "            \".\",\n",
    "            \",\",\n",
    "            \"\\u200b\",  # Zero-width space\n",
    "            \"\\uff0c\",  # Fullwidth comma\n",
    "            \"\\u3001\",  # Ideographic comma\n",
    "            \"\\uff0e\",  # Fullwidth full stop\n",
    "            \"\\u3002\",  # Ideographic full stop\n",
    "            \"\",\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    source, page_content = get_texts(file_path)\n",
    "    documents = text_splitter.create_documents([page_content])\n",
    "    return documents\n",
    "    \n",
    "#---End---document----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Start---upload----\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def upload_to_qdrant(docs: list[Document], user_id: str):\n",
    "    try:\n",
    "        Qdrant.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=embeddings,\n",
    "            url=QDRANT_URL,\n",
    "            api_key=QDRANT_API_KEY,\n",
    "            collection_name=user_id,\n",
    "            content_payload_key=CONTENT_PAYLOAD_KEY,\n",
    "            metadata_payload_key=METADATA_PAYLOAD_KEY,\n",
    "        )\n",
    "    except TimeoutException as e:\n",
    "        print(f\"Timeout occurred: {e}\")\n",
    "        raise \n",
    "\n",
    "async def upload_common(file_abs_path: str, user_id: str) -> bool:\n",
    "    docs = split_documents_file_common(file_abs_path)\n",
    "\n",
    "    for i in range(0, len(docs), BATCH_SIZE_UPLOAD):\n",
    "        batch = docs[i:i + BATCH_SIZE_UPLOAD]\n",
    "        try:\n",
    "            upload_to_qdrant(batch, user_id)\n",
    "        except TimeoutException:\n",
    "            print(f\"Failed to upload batch {i // BATCH_SIZE_UPLOAD + 1}. Moving to the next batch.\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "    \n",
    "async def upload_soc(file_abs_path: str, user_id: str):\n",
    "    # completed_positions = {}\n",
    "    # docs = split_documents_file_soc(file_abs_path)\n",
    "    \n",
    "    # print(\"Length\", len(docs))\n",
    "    # print(\"File name\", file_abs_path)\n",
    "    \n",
    "    # for idx, doc in enumerate(docs):\n",
    "    #     template = get_evaluate_template(TODO_LIST_STR, doc)\n",
    "    #     print(\"Template size:\", len(template))\n",
    "        \n",
    "    #     async for chunk in generate_stream_str(template):\n",
    "    #         completed_position_list = process_result_of_evaluate(chunk)\n",
    "            \n",
    "    #         if completed_position_list == []: \n",
    "    #             continue\n",
    "            \n",
    "    #         for completed_position in completed_position_list:\n",
    "    #             completed_positions[completed_position] = doc\n",
    "        \n",
    "    #     create_information_security_evaluation(TODO_LIST, completed_positions, user_id)\n",
    "    #     print(\"Done index: \", idx)\n",
    "            \n",
    "    # file_evaluation_name, pdf_output_path = create_information_security_evaluation(TODO_LIST, completed_positions, user_id)\n",
    "    file_evaluation_name = \"Information_Security_Evaluation.pdf\"\n",
    "    pdf_output_path = \"./soc-data-store/user1/Information_Security_Evaluation.pdf\"\n",
    "    # print(\"Completed positions\", completed_positions)\n",
    "    print(f\"PDF output path: {pdf_output_path}\")\n",
    "    \n",
    "    return file_evaluation_name, pdf_output_path\n",
    "\n",
    "async def upload(file: FileStorage, user_id: str, save_file: bool = True):\n",
    "    result_upload_common = None\n",
    "    result_upload_soc = None\n",
    "    \n",
    "    file_abs_path = await save_pdf(file, user_id)\n",
    "    print(\"1\")\n",
    "    result_upload_common = await upload_common(file_abs_path, user_id)\n",
    "    print(\"2\")\n",
    "    \n",
    "    docs = split_documents_file_common(file_abs_path)\n",
    "    sample_content = docs[0]\n",
    "    print(sample_content)\n",
    "    \n",
    "    is_socreport_file = await check_is_socreport_file(sample_content)\n",
    "    print(\"is\", is_socreport_file)\n",
    "    if is_socreport_file:\n",
    "        print(\"Hehe\")\n",
    "        result_upload_soc = await upload_soc(file_abs_path, user_id)\n",
    "    \n",
    "    if os.path.exists(file_abs_path) and not save_file: \n",
    "        print(\"434\")\n",
    "        os.remove(file_abs_path)\n",
    "        \n",
    "    return (result_upload_common, result_upload_soc, is_socreport_file)\n",
    "\n",
    "#---End---upload----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Start---API----\n",
    "\n",
    "@app.post(\"/llms\")\n",
    "async def post_llms(template: str = Form(...)):\n",
    "    return StreamingResponse(\n",
    "        generate_stream_json(template), media_type='text/event-stream'\n",
    "    )\n",
    "\n",
    "@app.post(\"/ask\")\n",
    "async def post_ask(user_id: str = Form(...), user_query: str = Form(...)):\n",
    "    template = await get_ask_template(user_id, user_query)\n",
    "    print(\"ask\", user_id)\n",
    "    print(\"ask\", user_query)\n",
    "    \n",
    "    return StreamingResponse(\n",
    "        generate_stream_json(template), media_type='text/event-stream'\n",
    "    )\n",
    "\n",
    "@app.post(\"/upload\")\n",
    "async def post_upload(user_id: str = Form(...), file: UploadFile = File(...)):\n",
    "    print(\"upload-common\", user_id)\n",
    "    print(\"upload-common\", file.filename)\n",
    "    \n",
    "    if file and user_id:\n",
    "        result_upload_common, result_upload_soc, is_socreport_file = await upload(file, user_id, True)\n",
    "        print(\"result_upload_common, result_upload_soc, is_socreport_file\", result_upload_common, result_upload_soc, is_socreport_file)\n",
    "        if is_socreport_file:\n",
    "            file_evaluation_name, pdf_output_path = result_upload_soc\n",
    "            print(\"file_evaluation_name, pdf_output_path\", file_evaluation_name, pdf_output_path)\n",
    "            return FileResponse(pdf_output_path, media_type='application/pdf', filename=file_evaluation_name)\n",
    "    \n",
    "    return JSONResponse(\n",
    "        content={\n",
    "            \"response\": \"Tải lên file không thành công, vui lòng kiểm tra lại file pdf của bạn!\",\n",
    "            \"success\": False\n",
    "        },\n",
    "        status_code=200\n",
    "    )\n",
    "\n",
    "#---End---API----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java.net.SocketException: Connection reset\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n",
      "\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n",
      "\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)\n",
      "\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\n",
      "\tat org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)\n",
      "\tat org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)\n",
      "\tat org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)\n",
      "\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)\n",
      "\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.recv_interpret(RemoteInterpreterService.java:274)\n",
      "\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.interpret(RemoteInterpreterService.java:258)\n",
      "\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter$4.call(RemoteInterpreter.java:233)\n",
      "\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter$4.call(RemoteInterpreter.java:229)\n",
      "\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:135)\n",
      "\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:228)\n",
      "\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:449)\n",
      "\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n",
      "\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:315)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    config = Config(\n",
    "        app=app,\n",
    "        host=\"0.0.0.0\",\n",
    "        port=7722,\n",
    "        log_level=\"debug\",\n",
    "        log_config=None,\n",
    "        use_colors=False\n",
    "    )\n",
    "    \n",
    "    server = Server(config)\n",
    "    await server.serve()\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
